= SINGLE CELL PORTAL README

== SETUP

This application is built and deployed using Docker[https://www.docker.com], specifically native {Docker for Mac OSX}[https://docs.docker.com/docker-for-mac/].
Please refer to their online documentation for instructions on installing and creating a default VM for managing Docker images.

== BUILDING THE DOCKER IMAGE

Once all source files are checked out and Docker has been installed and your VM configured, open a terminal window and execute the following steps:

1. Navigate to the project directory
2. Build the Single Cell Portal image: <code>docker build -t single_cell_docker -f Dockerfile .</code>

This will start the automated process of building the Docker image for running the portal.  The image is built off of
the {Passenger-docker baseimage}[https://github.com/phusion/passenger-docker] and comes with Ruby, Nginx, and Passenger
by default, with additional packages added to the {Broad Institute KDUX Rails baseimage}[https://hub.docker.com/r/broadinstitute/kdux-rails-baseimage/]
which pulls from the original baseimage.  The extended image contains Oracle Instant Client, ImageMagick, and Sphinx Search.

<em>If this is your first time building the image, it may take several minutes to download and install everything.</em>

== BEFORE RUNNING THE CONTAINER

Since this project utilizes native Docker for Mac OSX, any resources on the host machine cannot be reached by the running
container (specifically, any database resources). Therefore, we will need to deploy a database container using Docker
as well.  This project uses {MongoDB}[https://hub.docker.com/_/mongo/] as the primary datastore.

First, create a directory somewhere on your computer in which to store the raw database content (it doesn't matter where
as long as it has <code>rw</code> permissions, but preferably it would be inside your home directory).

To deploy the database container:

1. Pull the image: <code>docker pull mongo</code>
2. Navigate to the project directory
3. Run the helper script to start the DB container: <code>bin/boot_mongo -d (path to data store directory)</code>

Note: Once the container has been run once, you can stop & restart it using: <code>docker stop mongodb</code> or
<code>docker restart mongodb</code>

== RUNNING THE CONTAINER

Once the image has successfully built and the database container is running, use the following command to start the container:
  bin/boot_docker -u (sendgrid username) -P (sendgrid password) -k (service account key path)
<em>Contact bistline@broadinstitute.org[mailto:bistline@broadinstitute.org] for credentials to use in development and production.</em>

This sets up several environment variables in your shell and then runs the following command:
  docker run --rm -it --name $CONTAINER_NAME -p 80:80 -p 443:443 -p 587:587 --link mongodb:mongodb -h localhost -v $PROJECT_DIR:/home/app/webapp:rw -e PASSENGER_APP_ENV=$PASSENGER_APP_ENV -e MONGO_LOCALHOST=$MONGO_LOCALHOST -e SENDGRID_USERNAME=$SENDGRID_USERNAME -e SENDGRID_PASSWORD=$SENDGRID_PASSWORD -e SECRET_KEY_BASE=$SECRET_KEY_BASE -e SERVICE_ACCOUNT_KEY=$SERVICE_ACCOUNT_KEY single_cell_docker

The container will then start running, and will execute its local startup scripts that will configure the application automatically.

You can also run the <code>bin/boot_docker</code> script in help mode by passing <code>-H</code> to print the help text
which will show you how to pass specific values to the above env variables.  <em>Note: running the shortcut script with
an environment of 'production' will cause the container to spawn headlessly by passing the <code>-d</code> flag, rather
than <code>--rm -it</code>.</em>

=== DOCKER RUN COMMAND ENVIRONMENT VARIABLES
There are several variables that need to be passed to the Docker container in order to run properly:
1. *CONTAINER_NAME* (passed with --name): This names your container to whatever you want.  This is useful when linking containers.
3. *PROJECT_DIR* (passed with -v): This mounts your local working directory inside the Docker container.  Makes doing local development via hot deployment possible.
4. *PASSENGER_APP_ENV* (passed with -e): The Rails environment you wish to load.  Can be either development or production (default is development).
5. *MONGO_LOCALHOST* (passed with -e): Name of the container running MongoDB.  Even though our two containers are linked, this needs to be set to allow Rails to communicate with the database.
6. *SENDGRID_USERNAME* (passed with -e): The username associated with the Sendgrid account (for sending emails).
7. *SENDGRID_PASSWORD* (passed with -e): The password associated with the Sendgrid account (for sending emails).
8. *SECRET_KEY_BASE* (passed with -e): Sets the Rails SECRET_KEY_BASE environment variable, used mostly by Devise in authentication for cookies.
9. *SERVICE_ACCOUNT_KEY* (passed with -e): Sets the Rails SERVICE_ACCOUNT_KEY environment variable, used for making authenticated API calls to FireCloud & GCP.

=== RUN COMMAND IN DETAIL
The run command explained in its entirety:
* *--rm:* This tells Docker to automatically clean up the container after exiting.
* *-it:* Leaves an interactive shell running in the foreground where the output of Nginx can be seen.
* <b>--name CONTAINER_NAME:</b> This names your container to whatever you want.  This is useful when linking other Docker containers to the portal container, or when connecting to a running container to check logs or environment variables.
* <b>-p 80:80 -p 443:443 -p 587:587:</b> Maps ports 80 (HTTP), 443 (HTTPS), and 587 (smtp) on the host machine to the corresponding ports inside the Docker container.
* <b>--link mongodb:mongodb</b>: Connects our webapp container to the mongodb container, creating a virtual hostname inside the single_cell_docker container called mongodb.
* <b>-v [PROJECT_DIR]/:/home/app/webapp:</b> This mounts your local working directory inside the running Docker container in the correct location for the portal to run.  This accomplishes two things:
  - Enables hot deployment for local development
  - Persists all project data past destruction of Docker container (since we're running with --rm), but not system-level log or tmp files.
* <b>-e PASSENGER_APP_ENV= [RAILS_ENV]:</b> The Rails environment.  Will default to development, so if you're doing a production deployment, set this accordingly.
* <b>-e MONGO_LOCALHOST= [MONGO_LOCALHOST]:</b> Name of the container running MongoDB.  Even though our two containers are linked, this needs to be set to allow Rails to communicate with the database.
* <b>-e SENDGRID_USERNAME= [SENDGRID_USERNAME] -e SENDGRID_PASSWORD= [SENDGRID_PASSWORD]:</b> The credentials for Sendgrid to send emails.  Alternatively, you could decide to not use Sendgrid and configure the application to use a different SMTP server (would be done inside your environment's config file).
* <b>-e SECRET_KEY_BASE= [SECRET_KEY_BASE]:</b> Setting the SECRET_KEY_BASE variable is necessary for creating secure cookies for authentication.  This variable automatically resets every time we restart the container.
* <b>-e SERVICE_ACCOUNT_KEY= [SERVICE_ACCOUNT_KEY]:</b> Setting the SERVICE_ACCOUNT_KEY variable is necessary for making authenticated API calls to FireCloud and GCP.  This should be a file path <b>relative to the app root</b> that points to the JSON service account key file you exported from GCP.
* *single_cell_docker*: This is the name of the image we created earlier.  If you chose a different name, please use that here.

=== MEMORY CONSTRAINTS

When the portal and database are under load, and memory usage is of a concern (more in production than in local development)
as file parsing is resource-intensive.  To address this, running <code>bin/boot_docker -e production</code> will enable
memory limits (passed via <code>docker run -m</code>) to prevent either the portal or the database from running out of
memory and being killed by the host OS.  In production, by default the Google VM is configured with 4 cores and 26GB of
RAM.  Therefore, the memory defaults in both <code>boot_docker</code> and <code>boot_mongo</code> have been set to keep
total memory usage under this limit.

Memory defaults are:
* Portal memory: 12GB (production only)
* MongoDB memory: 12GB (always enabled)
* MongoDB WiredTiger cache size: 4GB (always enabled)

These defaults can be overridden inside both <code>boot_docker</code> and <code>boot_mongo</code> by using the appropriate
flags.  To see these, run <code>bin/boot_docker -H</code> and <code>boot_mongo -H</code>.  Depending on your environment,
you may wish to override these depending on resources available.

=== ADMIN USER ACCOUNTS
The Single Cell Portal has the concept of a 'super-admin' user account, which will allow portal admins to view & edit any
studies in the portal for QA purposes, as well as receive certain admin-related emails.  This can only be enabled manually
through the console.

To create an admin user account:
* Create a user account normally through the UI (using sign-up or Google auth features)
* Start the portal locally (or ssh into production VM)
* Connect to the running portal container: <code>docker exec -it single_cell bash</code>
* Query for the desired user account: <code>user = User.find_by(email: '<email address here>')</code>
* Set the admin flag to true: <code>user.update(admin: true)</code>

== TESTS

All tests are handle through {Selenium Webdriver}[http://www.seleniumhq.org/docs/03_webdriver.jsp] and Chromedriver[https://sites.google.com/a/chromium.org/chromedriver/]
and are run against a regular instance of the portal, usually in development mode.  The test suite is run from the
<code>test/ui_test_suite.rb</code> script.

Due to the nature of Docker, the tests cannot be run from inside the container as the Docker image cannot connect back to Chromedriver
and the display from inside the VM.  Therefore, you will need to have a minimal portal environment enabled outside of Docker. The
minumum requirements are as follows:

* Ruby >= 2.3, preferably mangaged through RVM[https://rvm.io/] or rbenv[https://github.com/rbenv/rbenv]
* Gems: rubygems[https://github.com/rubygems/rubygems], test-unit[http://test-unit.github.io], selenium-webdriver[https://github.com/SeleniumHQ/selenium/tree/master/rb] (see Gemfile.lock for version requirements)
* Google Chrome with 2 Google accounts already signed in, one of which needs to be a portal admin account (see above)
* Chromedriver
* FireCloud accounts for both Google accounts used above

To launch the test suite, run the following command in the portal root directory:

  ruby test/ui_test_suite.rb -- -e=(email account #1) -s=(email account #2)

The test suite will use your default Chrome profile to load Google accounts so that you do not need to authenticate in manually.
This can be configured with the <code>-p</code> flag as such:

  ruby test/ui_test_suite.rb -- -e=(email account #1) -s=(email account #2) -p=/path/to/chrome/profile

Paths to the chromedriver binary and your profile download directory can also be configured with the <code>-c</code> and <code>-d</code> flags, respectively.

There are 3 main groups of tests: admin (study creation & editing), front-end (study data searching & visualization), and cleanup
(removing studies created during testing).  You can run groups discretely by passing <code>-n /pattern/</code> to the test suite as follows:

To run all admin tests:

  ruby test/ui_test_suite.rb -n /admin/ -- (rest of test parameters)

To run all front-end tests:

  ruby test/ui_test_suite.rb -n /front-end/ -- (rest of test parameters)

This can also be used to run single tests, or smaller groups by refining the regular expression to match only certain names
of tests.  For example, to run all front-end tests that deal with file downloads:

  ruby test/ui_test_suite.rb -n /front-end.*download/ -- (rest of test parameters)

More information on usage & test configuration can be found in the comments at the top of the test suite.

== GOOGLE DEPLOYMENT
The production Single Cell portal is deployed in Google Cloud Platform.  The project name is *broad-singlecellportal*.
If you need to access the production instance for maintenance purposes:
* Go to the broad-singlecellportal[https://console.cloud.google.com/home/dashboard?project=broad-singlecellportal] GCP page
* Select "Compute Engine" from the top-left nav dropdown
* At the bottom of the page is the entry for the production VM (called singlecell-production)
* Click the SSH button under the Connect heading (this will launch an SSH tunnel in a browser window)

Once you have connected, the portal is running in this directory: <code>/home/docker-user/single_cell_portal</code>.
All source files are owned by <code>docker-user</code>, so if you need to pull from git, make sure you
<code>sudo -u docker-user -Hs</code> first to preserve the correct file permissions.  Any docker commands need to be run
as <code>root</code>, so exit out of the shell before running them.

The production URL is: https://portals.broadinstitute.org/single_cell

=== FIRECLOUD INTEGRATION

The Single Cell Portal stores uploaded study data files in FireCloud[https://software.broadinstitute.org/firecloud/] workspaces,
which in turn store data in GCP buckets.  This is all managed through a GCP service account which in turn owns all portal workspaces
and manages them on behalf of portal users.  All portal-related workspaces are within the <code>single-cell-portal</code>
namespace, which should be noted is a separate project from the one the portal operates out of.

When a study is created through the portal, a call is made to the FireCloud API to provision a workspace and set the ACL to allow
owner access to the user who created the study, and read/write access to any designated shares.  Every FireCloud workspace comes
with a GCP storage bucket, which is where all uploaded files are deposited.  No ACLs are set on individual files as all permissions
are inherited from the workspace itself.  Files are first uploaded temporarily locally to the portal (so that they can be parsed
if needed) and then sent to the workspace bucket in the background after uploading and parsing have completed.

Deleting a study will also delete the associated workspace, unless the user specifies that they want the workspace to be persisted.
New studies can also be initialized from an existing workspace (specified by the user during creation) which will synchronize all
files and permissions.

==== ADMIN CONTROL PANEL, DOWNLOAD QUOTAS & ACCESS REVOCATION

All portal users are required to authenticate before downloading data as we implement daily per-user quotas.  These are configurable
through the admin control panel which can be accessed only by portal admin accounts (available through the profile menu or at /single_cell/admin).

There are currently 2 configuration options:

* Daily download quota limit (defaults to 2 terabytes, but is configurable to any amount, including 0)
* Disabling all downloads

Disabling all downloads is achieved by revoking all access to studies directly in FireCloud and using the portal permission map
(study ownership & shares) as a backup cache.  This will prevent anyone from downloading data either through the portal or directly from the
workspaces themselves.  This will have the side effect of disallowing any edits to studies while in effect, so this feature should only
be used as a last resort to curtail runaway downloads.  While downloads are disabled, only the portal service account will have
access to workspaces.

Re-enabling downloads will restore all permissions back to their previous state.

=== MAINTENANCE MODE

The production Single Cell portal has a defined maintenance window every <b>Monday from 9:30-10:30AM EST</b>.  To minimize
user dispruption when doing updates during that window (or hot fixes any other time) the portal has a 'maintenance mode'
feature that will return a 503 and redirect all incoming traffic to a static maintenance HTML page.

To use this feature, run the <code>bin/enable_maintenance.sh [on/off]</code> script accordingly.

=== PRODUCTION DOCKER COMMANDS

* To bounce MongoDB: <code>docker restart mongodb</code>
* To bounce the portal: <code>docker restart single_cell</code> <em>Note: MongoDB must be running to restart the portal, otherwise it cannot instantiate the link to the database.</em>
* To connect to the running MongoDB container: <code>docker exec -it mongodb mongo</code>
* To connect to the running portal container: <code>docker exec -it single_cell bash</code>

If you have pulled changes from source that require re-building the container, follow this checklist from inside the
project directory as <code>root</code>:
1. Rebuild the docker image: <code>docker build -t single_cell_docker -f Dockerfile .</code>
2. Stop the portal: <code>docker stop single_cell</code>
3. Remove the container instance: <code>docker rm single_cell</code>
4. Launch a new instance of the portal with the updated container:
  bin/boot_docker -u (sendgrid username) -P (sendgrid password) -e production -p (prod database password) -h portals.broadinstitute.org -k (service account key path)

<em>Contact bistline@broadinstitute.org[mailto:bistline@broadinstitute.org] for credentials to use with production</em>

<b>You will need to rebuild the docker image if you do any of the following:</b>
* Edit the Dockerfile
* Edit any scripts or configuration files listed in the Dockerfile:
  - Gemfile (including Gemfile.lock if versions have changed)
  - set_user_permissions.bash
  - rails_startup.bash
  - nginx.conf
  - webapp.conf

When you launch a new instance of the portal, you should get a response that is looks like a giant hexadecimal string -
this is the instance ID of the new container.  Once the container is running, you can connect to it with the <code>docker exec</code>
command and perform various Rails-specific actions, like:

* Re-index the database: <code>bin/rake RAILS_ENV=production db:mongoid:create_indexes</code>
* Launch the Rails console (to inspect database records, for instance): <code>bin/rails console production</code>